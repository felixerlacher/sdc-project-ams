{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_site(start_url, max_depth=10):\n",
    "    visited_urls = set()\n",
    "    to_visit = deque([(start_url, 0)])  # Queue of (url, depth)\n",
    "    all_text_content = []\n",
    "\n",
    "    while to_visit:\n",
    "        url, depth = to_visit.popleft()\n",
    "        if url in visited_urls or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        print(f\"Scraping {url}, Depth: {depth}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for JavaScript to load\n",
    "\n",
    "        # Extract text content\n",
    "        elements = driver.find_elements(By.XPATH, '//*')\n",
    "        text_content = [element.text.strip() for element in elements if element.text.strip() != '']\n",
    "        all_text_content.extend(text_content)\n",
    "\n",
    "        print(f\"Text found on {url}: {text_content[:5]}\")  # Print first 5 elements of text content for brevity\n",
    "\n",
    "        if depth < max_depth:\n",
    "            # Extract links and add them to the queue\n",
    "            links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "            hrefs = [link.get_attribute('href') for link in links if link.get_attribute('href') and 'http' in link.get_attribute('href')]\n",
    "            for href in hrefs:\n",
    "                to_visit.append((href, depth + 1))\n",
    "\n",
    "    return all_text_content\n",
    "\n",
    "# Start scraping from the main page\n",
    "all_text_content = scrape_site('https://www.ams.at/arbeitsuchende/')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Do something with the scraped data\n",
    "print(all_text_content[:500])  # Print first 500 characters of the result for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def accept_cookies(first_page):\n",
    "    if first_page:\n",
    "        try:\n",
    "            # Find the cookie accept button by its text and click it\n",
    "            accept_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Einverstanden')]\")\n",
    "            accept_button.click()\n",
    "            time.sleep(1)  # Wait for the banner to disappear\n",
    "        except Exception as e:\n",
    "            print(\"Attempt to find and click cookie banner failed:\", str(e))\n",
    "            \n",
    "\n",
    "def scrape_site(start_url, max_depth=10):\n",
    "    visited_urls = set()\n",
    "    to_visit = deque([(start_url, 0)])  # Queue of (url, depth)\n",
    "    all_text_content = []\n",
    "    first_page = True  # Flag to indicate the first page\n",
    "\n",
    "    while to_visit:\n",
    "        url, depth = to_visit.popleft()\n",
    "        if url in visited_urls or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        print(f\"Scraping {url}, Depth: {depth}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        accept_cookies(first_page)  # Attempt to accept cookies only on the first page\n",
    "        first_page = False  # Set the flag to False after the first page\n",
    "        time.sleep(2)  # Wait for JavaScript to load\n",
    "\n",
    "        # Extract text content\n",
    "        elements = driver.find_elements(By.XPATH, '//*')\n",
    "        text_content = [element.text.strip() for element in elements if element.text.strip() != '']\n",
    "        all_text_content.extend(text_content)\n",
    "\n",
    "        print(f\"Text found on {url}: {text_content[:5]}\")  # Print first 5 elements of text content for brevity\n",
    "\n",
    "        if depth < max_depth:\n",
    "            # Extract links and add them to the queue\n",
    "            links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "            hrefs = [link.get_attribute('href') for link in links if link.get_attribute('href') and 'http' in link.get_attribute('href')]\n",
    "            for href in hrefs:\n",
    "                to_visit.append((href, depth + 1))\n",
    "\n",
    "    return all_text_content\n",
    "\n",
    "# Start scraping from the main page\n",
    "all_text_content = scrape_site('https://www.ams.at/')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Do something with the scraped data\n",
    "print(all_text_content[:500])  # Print first 500 characters of the result for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def accept_cookies(first_page):\n",
    "    if first_page:\n",
    "        try:\n",
    "            # Find the cookie accept button by its text and click it\n",
    "            accept_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Einverstanden')]\")\n",
    "            accept_button.click()\n",
    "            time.sleep(1)  # Wait for the banner to disappear\n",
    "        except Exception as e:\n",
    "            print(\"Attempt to find and click cookie banner failed:\", str(e))\n",
    "\n",
    "def load_existing_data(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return set(file.read().splitlines())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def save_to_file(data, existing_data, filename='scraped_content.txt'):\n",
    "    # Filter out duplicates from the new data\n",
    "    unique_data = set(data) - existing_data\n",
    "\n",
    "    # Update existing data set\n",
    "    existing_data.update(unique_data)\n",
    "\n",
    "    # Write the unique data to the file\n",
    "    with open(filename, 'a', encoding='utf-8') as file:\n",
    "        for line in unique_data:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def scrape_site(start_url, max_depth=10):\n",
    "    visited_urls = set()\n",
    "    to_visit = deque([(start_url, 0)])  # Queue of (url, depth)\n",
    "    all_text_content = []\n",
    "    first_page = True  # Flag to indicate the first page\n",
    "    existing_data = load_existing_data('scraped_content.txt')\n",
    "\n",
    "    while to_visit:\n",
    "        url, depth = to_visit.popleft()\n",
    "        if url in visited_urls or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        print(f\"Scraping {url}, Depth: {depth}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        accept_cookies(first_page)\n",
    "        first_page = False\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*')))\n",
    "            elements = driver.find_elements(By.XPATH, '//*')\n",
    "            text_content = [element.text.strip() for element in elements if element.text.strip() != '']\n",
    "            all_text_content.extend(text_content)\n",
    "\n",
    "            save_to_file(text_content, existing_data)\n",
    "            print(f\"Text found on {url}: {text_content[:5]}\")  # Print first 5 elements of text content for brevity\n",
    "\n",
    "            if depth < max_depth:\n",
    "                links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "                hrefs = [link.get_attribute('href') for link in links if link.get_attribute('href') and 'http' in link.get_attribute('href')]\n",
    "                for href in hrefs:\n",
    "                    to_visit.append((href, depth + 1))\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"Timed out waiting for page elements to load at {url}\")\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Element not found on {url}\")\n",
    "\n",
    "    return all_text_content\n",
    "\n",
    "# Start scraping from the main page\n",
    "all_text_content = scrape_site('https://www.ams.at/')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Do something with the scraped data\n",
    "print(all_text_content[:500])  # Print first 500 characters of the result for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    unique_lines = set()\n",
    "    for line in lines:\n",
    "        # You can strip whitespace or do other processing if needed\n",
    "        unique_lines.add(line.strip())\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for line in unique_lines:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "# Replace 'inputfile.txt' with the path to your original text file\n",
    "# and 'outputfile.txt' with the path for the new file\n",
    "remove_duplicates('scraped_content.txt', 'scraped_content_uniquelines.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nGlossar » Informationen zu zentralen Themen | AMS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nZum Inhalt\\n\\n\\nZur AMS-Navigation\\n\\n\\n\\n\\n\\n\\nSind Sie damit einverstanden, dass wir anonymisiert Ihr Surfverhalten zur  Verbesserung aufzeichnen?\\nAMS.at verwendet Cookies um Ihnen das bestmögliche Surferlebnis zu\\xa0 ermöglichen. Um auch weiterhin unseren Webauftritt besser zu gestalten, analysieren wir anonymisiert das Surfverhalten unserer Nutzer und\\xa0 Nutzerinnen. Sie können dies jederzeit in den Privatsphäre-Einstellungen anpassen.\\n\\nMehr darüber erfahren...\\n\\n\\nEinverstanden\\nNicht Einverstanden\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nZu welchen Inhalten möchten Sie?\\n\\n\\n\\nInhalte für:\\n\\n\\nArbeitsuchende\\n\\n\\n\\nUnternehmen\\n\\n\\n\\nArbeitsmarktdaten und Medien\\n\\n\\n\\nOrganisation\\n\\n\\n\\n\\n\\nIn Bundesland:\\n\\n\\n\\nBurgenland\\n\\n\\nKärnten\\n\\n\\nNiederösterreich\\n\\n\\nOberösterreich\\n\\n\\nSalzburg\\n\\n\\nSteiermark\\n\\n\\nTirol\\n\\n\\nVorarlberg\\n\\n\\nWien\\n\\n\\n---\\nÖsterreich\\n\\n\\n\\n\\n\\n\\nAuswahl bestätigen\\nAbbrechen\\n\\n\\n\\n\\n\\n\\nA\\nA\\nA\\n\\n\\n\\n\\nSuche starten\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nÜber AMS\\n\\nPartner_innen\\n\\nGeschäftsberichte\\n\\nKarriere beim AMS\\n\\nVeranstaltungen\\n\\nAdressen und Telefonnummern\\n\\nKontaktmöglichkeiten\\n\\nGlossar\\n\\nArbeitsuchende\\n\\nUnternehmen\\n\\nAMS eServices\\n\\nFormulare\\n\\nFörder Wiki\\n\\n\\n\\nMehr\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSie sind hier:\\n\\n\\nOrganisation\\n\\n\\nGlossar\\n\\n\\n\\n\\nGlossar\\n\\n\\n\\n\\n \\n\\n\\tHier finden Sie Informationen zu zentralen Themen, die Arbeitssuchende, Unternehmen und ganz allgemein das AMS betreffen.  \\n\\n\\n\\n\\n\\n\\nSuchen\\n\\nEs wurden keine Informationen gefunden \\n\\n\\n\\n\\n\\t        A\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        AUFLEB\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\nVerein zur Ausbildung und Unterstützung von Arbeitslosen\\n\\n\\n\\n\\n\\n\\t        D\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        Digi-Winner\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tdigi-winner//\\n\\t\\t    \\n\\t\\t    \\tdigi winner//\\n\\t\\t    \\n\\t\\t    \\tDigi Winner//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nFörderungen von Arbeiterkammer und Stadt Wien für Weiterbildung im digitalen Bereich. Für alle arbeitslosen Wiener_innen, die AK-Mitglied sind.\\n\\nNäheres unter:\\xa0https://www.waff.at/foerderungen/digi-winner/digi-winner-arbeitslose/\\n\\xa0\\n\\n\\n\\n\\n\\n\\t        E\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        EWR\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tEWR-Raum//\\n\\t\\t    \\n\\t\\t    \\tEWR-Land//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nEWR steht für \"Europäischer Wirtschaftsraum\". Zum EWR gehören die Mitgliedsstaten der EU sowie Island, Liechtenstein und Norwegen.\\n\\n\\n\\n\\n\\n\\t        F\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        Finanzielle Notlage\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tfinanziellen Notlage//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nOb und wann eine finanzielle Notlage vorliegt kann nur in einem individuellen Gespräch mit Ihrer Beraterin oder Ihrem Berater geklärt werden. Bitte haben Sie Verständnis.\\n\\n\\n\\n\\n\\n\\t        G\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        Green Jobs\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tgreenjobs//\\n\\t\\t    \\n\\t\\t    \\tGreen-Jobs//\\n\\t\\t    \\n\\t\\t    \\tgreen jobs//\\n\\t\\t    \\n\\t\\t    \\tGreen Jobs//\\n\\t\\t    \\n\\t\\t    \\tGreenjobs//\\n\\t\\t    \\n\\t\\t    \\tgreen-jobs//\\n\\t\\t    \\n\\t\\t    \\t\"green jobs\"//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nAls\\xa0Green Jobs\\xa0werden Arbeitsplätze im Umweltsektor bezeichnet. Laut Definition der Europäischen Union (EU) sind\\xa0Green Jobs\\xa0Arbeitsplätze in der Herstellung von Produkten, Technologien und Dienstleistungen, die Umweltschäden vermeiden und natürliche\\xa0Ressourcen\\xa0erhalten. Diese Arbeitsplätze findet man in den verschiedensten Sparten wie zum Beispiel erneuerbare Energien, nachhaltiges Bauen und Sanieren sowie Wasser- und Abwassermanagement. Berufe mit hohem Qualifikationsniveau können ebenso dazugehören wie Lehrberufe oder Hilfsarbeiten. Der Hauptzweck von\\xa0Green Jobs\\xa0ist der Beitrag zum Umweltschutz. Daher können in allen wirtschaftlichen und gesellschaftlichen Bereichen\\xa0Green Jobs\\xa0gefunden werden\\xa0bzw.\\xa0können sich bestehende Berufsbilder zu\\xa0Green Jobs\\xa0wandeln.\\n\\n\\n\\n\\n\\n\\n\\t        J\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        junge Erwachsenen\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tjungen Erwachsene//\\n\\t\\t    \\n\\t\\t    \\tjungen Erwachsenen//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nPersonen im Alter von 18 bis 25 Jahren\\n\\n\\n\\n\\n\\n\\t        K\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        Kleine und mittlere Unternehmen\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tKMU//\\n\\t\\t    \\n\\t\\t    \\tKlein- und Mittelbetriebe//\\n\\t\\t    \\n\\t\\t    \\tKlein- und Mittelständische Betriebe//\\n\\t\\t    \\n\\t\\t    \\tKleinunternehmen//\\n\\t\\t    \\n\\t\\t    \\tmittlere Unternehmen//\\n\\t\\t    \\n\\t\\t    \\tkleine Unternehmen//\\n\\t\\t    \\n\\t\\t    \\tkleine und mittlere Unternehmen//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nAls Klein- und Mittelunternehmen (KMU) gelten Unternehmen, die weniger als 250 Personen beschäftigen und einen Jahresumsatz von höchstens 50 Mio. Euro oder eine Jahresbilanzsumme von höchstens 43 Mio. Euro haben und sich höchstens zu 26 % im Besitz einem oder mehreren Unternehmen befinden, welche nicht als KMU definert sind.\\n\\n\\n\\n\\n\\n\\t        N\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        NAL\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\nLangzeitbeschäftigungslos -\\xa0Personen, die am Stichtag mindestens 366 Tage Netto-ArbeitsLosigkeits-Dauer aufweisen (Netto = mit kurzen Unterbrechungen).\\n\\n\\n\\n\\n\\n\\t        R\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        Rechtsanspruch auf berufliche Maßnahmen der Rehabilitation\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\nEin Rechtsanspruch auf berufliche Maßnahmen der Rehabilitation ergibt sich entweder aus § 253e, 270 oder 276e des Allgemeinen Sozialversicherungsgesetzes (ASVG).\\n\\n\\n\\n\\n\\n\\t        W\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        waff\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \\tWAFF//\\n\\t\\t    \\n\\t\\t    \\tWaff//\\n\\t\\t    \\n\\t\\t    \"\\n\\t    \\n\\nWiener ArbeitnehmerInnen Förderungsfonds\\n\\n\\n\\n\\n\\n\\t        Ü\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\n\\n\\n\\n\\n\\n\\t        ÜBA-TN\\n\\t    \\n\\n\\t   \\t\\t\"\\n\\t\\t\\t\\n\\t\\t    \"\\n\\t    \\n\\nTeilnehmer oder Teilnehmerin an einer Überbetrieblichen Lehrausbildung\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDiese Seite wurde aktualisiert am: 11. März 2020\\nWollen Sie einen Fehler melden?\\n\\nSeite drucken\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeite per E-Mail senden - es wird das Standard E-Mail Programm verwendet\\n\\n\\n\\n\\n\\nAuf Twitter teilen\\n\\n\\n\\n\\n\\nAuf Facebook teilen\\n\\n\\n\\n\\n\\n\\n\\nServices und Leistungen des AMS\\n\\n\\nJob-Suche\\n\\n\\nAMS Job App\\n\\n\\neAMS-Konto\\n\\n\\nArbeitslos melden\\n\\n\\nFormulare\\n\\n\\n\\n\\n\\n\\nKontakt zum AMS\\n\\n\\nKontakt\\n\\n\\nAMS Geschäftsstellen\\n\\n\\nBankverbindungen\\n\\n\\nOmbudsstellen\\n\\n\\nForschung\\n\\n\\nMedien\\n\\n\\n\\n\\n\\n\\nRegionale AMS-Seiten\\n\\n\\nWien\\n\\n\\nNiederösterreich\\n\\n\\nBurgenland\\n\\n\\nSalzburg\\n\\n\\nOberösterreich\\n\\n\\nTirol\\n\\n\\nVorarlberg\\n\\n\\nSteiermark\\n\\n\\nKärnten\\n\\n\\nÖsterreichweit\\n\\n\\n\\n\\n\\n\\nRechtliches\\n\\n\\nImpressum\\n\\n\\nDatenschutz\\n\\n\\nBarrierefreiheit\\n\\n\\nAllgemeine Geschäftsbedingungen\\n\\n\\nAMS Richtlinien\\n\\n\\nVeröffentlichungspflicht gem. Art. 20 Abs. 5 B-VG\\n\\n\\n\\n\\n\\n\\n\\n\\nSeitenübersicht\\n\\n\\nPrivatsphäre-Einstellungen\\n\\n\\nEnglish Version\\n\\n\\n\\n\\n\\n\\n          © 2024 AMS Österreich\\n\\n        \\nNach oben\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
