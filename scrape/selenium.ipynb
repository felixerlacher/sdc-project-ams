{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_site(start_url, max_depth=10):\n",
    "    visited_urls = set()\n",
    "    to_visit = deque([(start_url, 0)])  # Queue of (url, depth)\n",
    "    all_text_content = []\n",
    "\n",
    "    while to_visit:\n",
    "        url, depth = to_visit.popleft()\n",
    "        if url in visited_urls or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        print(f\"Scraping {url}, Depth: {depth}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for JavaScript to load\n",
    "\n",
    "        # Extract text content\n",
    "        elements = driver.find_elements(By.XPATH, '//*')\n",
    "        text_content = [element.text.strip() for element in elements if element.text.strip() != '']\n",
    "        all_text_content.extend(text_content)\n",
    "\n",
    "        print(f\"Text found on {url}: {text_content[:5]}\")  # Print first 5 elements of text content for brevity\n",
    "\n",
    "        if depth < max_depth:\n",
    "            # Extract links and add them to the queue\n",
    "            links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "            hrefs = [link.get_attribute('href') for link in links if link.get_attribute('href') and 'http' in link.get_attribute('href')]\n",
    "            for href in hrefs:\n",
    "                to_visit.append((href, depth + 1))\n",
    "\n",
    "    return all_text_content\n",
    "\n",
    "# Start scraping from the main page\n",
    "all_text_content = scrape_site('https://www.ams.at/arbeitsuchende/')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Do something with the scraped data\n",
    "print(all_text_content[:500])  # Print first 500 characters of the result for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def accept_cookies(first_page):\n",
    "    if first_page:\n",
    "        try:\n",
    "            # Find the cookie accept button by its text and click it\n",
    "            accept_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Einverstanden')]\")\n",
    "            accept_button.click()\n",
    "            time.sleep(1)  # Wait for the banner to disappear\n",
    "        except Exception as e:\n",
    "            print(\"Attempt to find and click cookie banner failed:\", str(e))\n",
    "            \n",
    "\n",
    "def scrape_site(start_url, max_depth=10):\n",
    "    visited_urls = set()\n",
    "    to_visit = deque([(start_url, 0)])  # Queue of (url, depth)\n",
    "    all_text_content = []\n",
    "    first_page = True  # Flag to indicate the first page\n",
    "\n",
    "    while to_visit:\n",
    "        url, depth = to_visit.popleft()\n",
    "        if url in visited_urls or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        print(f\"Scraping {url}, Depth: {depth}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        accept_cookies(first_page)  # Attempt to accept cookies only on the first page\n",
    "        first_page = False  # Set the flag to False after the first page\n",
    "        time.sleep(2)  # Wait for JavaScript to load\n",
    "\n",
    "        # Extract text content\n",
    "        elements = driver.find_elements(By.XPATH, '//*')\n",
    "        text_content = [element.text.strip() for element in elements if element.text.strip() != '']\n",
    "        all_text_content.extend(text_content)\n",
    "\n",
    "        print(f\"Text found on {url}: {text_content[:5]}\")  # Print first 5 elements of text content for brevity\n",
    "\n",
    "        if depth < max_depth:\n",
    "            # Extract links and add them to the queue\n",
    "            links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "            hrefs = [link.get_attribute('href') for link in links if link.get_attribute('href') and 'http' in link.get_attribute('href')]\n",
    "            for href in hrefs:\n",
    "                to_visit.append((href, depth + 1))\n",
    "\n",
    "    return all_text_content\n",
    "\n",
    "# Start scraping from the main page\n",
    "all_text_content = scrape_site('https://www.ams.at/')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Do something with the scraped data\n",
    "print(all_text_content[:500])  # Print first 500 characters of the result for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def accept_cookies(first_page):\n",
    "    if first_page:\n",
    "        try:\n",
    "            # Find the cookie accept button by its text and click it\n",
    "            accept_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Einverstanden')]\")\n",
    "            accept_button.click()\n",
    "            time.sleep(1)  # Wait for the banner to disappear\n",
    "        except Exception as e:\n",
    "            print(\"Attempt to find and click cookie banner failed:\", str(e))\n",
    "\n",
    "def load_existing_data(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return set(file.read().splitlines())\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "def save_to_file(data, existing_data, filename='scraped_content.txt'):\n",
    "    # Filter out duplicates from the new data\n",
    "    unique_data = set(data) - existing_data\n",
    "\n",
    "    # Update existing data set\n",
    "    existing_data.update(unique_data)\n",
    "\n",
    "    # Write the unique data to the file\n",
    "    with open(filename, 'a', encoding='utf-8') as file:\n",
    "        for line in unique_data:\n",
    "            file.write(line + '\\n')\n",
    "\n",
    "def scrape_site(start_url, max_depth=10):\n",
    "    visited_urls = set()\n",
    "    to_visit = deque([(start_url, 0)])  # Queue of (url, depth)\n",
    "    all_text_content = []\n",
    "    first_page = True  # Flag to indicate the first page\n",
    "    existing_data = load_existing_data('scraped_content.txt')\n",
    "\n",
    "    while to_visit:\n",
    "        url, depth = to_visit.popleft()\n",
    "        if url in visited_urls or depth > max_depth:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        print(f\"Scraping {url}, Depth: {depth}\")\n",
    "\n",
    "        driver.get(url)\n",
    "        accept_cookies(first_page)  # Attempt to accept cookies only on the first page\n",
    "        first_page = False  # Set the flag to False after the first page\n",
    "        time.sleep(2)  # Wait for JavaScript to load\n",
    "\n",
    "        # Extract text content\n",
    "        elements = driver.find_elements(By.XPATH, '//*')\n",
    "        text_content = [element.text.strip() for element in elements if element.text.strip() != '']\n",
    "        all_text_content.extend(text_content)\n",
    "\n",
    "        # Save the scraped content to file\n",
    "        save_to_file(text_content, existing_data)\n",
    "\n",
    "        print(f\"Text found on {url}: {text_content[:5]}\")  # Print first 5 elements of text content for brevity\n",
    "\n",
    "        if depth < max_depth:\n",
    "            # Extract links and add them to the queue\n",
    "            links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "            hrefs = [link.get_attribute('href') for link in links if link.get_attribute('href') and 'http' in link.get_attribute('href')]\n",
    "            for href in hrefs:\n",
    "                to_visit.append((href, depth + 1))\n",
    "\n",
    "    return all_text_content\n",
    "\n",
    "# Start scraping from the main page\n",
    "all_text_content = scrape_site('https://www.ams.at/')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Do something with the scraped data\n",
    "print(all_text_content[:500])  # Print first 500 characters of the result for brevity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
