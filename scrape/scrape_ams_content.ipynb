{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "# Reload the variables in your '.env' file (override the existing variables)\n",
    "dotenv.load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport requests\\nfrom bs4 import BeautifulSoup\\nfrom urllib.parse import urljoin, urlparse\\n\\ndef get_links(url, visited):\\n    \"\"\"Fetches all unique links on a given webpage\"\"\"\\n    try:\\n        response = requests.get(url)\\n        soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        links = set()\\n        for link in soup.find_all(\\'a\\', href=True):\\n            full_link = urljoin(url, link[\\'href\\'])\\n            if urlparse(full_link).netloc == urlparse(url).netloc and full_link not in visited:  # same domain and not visited\\n                links.add(full_link)\\n        return links\\n    except requests.exceptions.RequestException as e:\\n        print(f\"Error fetching {url}: {e}\")\\n        return set()\\n\\ndef save_content(url, folder):\\n    \"\"\"Saves the content of a URL to a file in the specified folder\"\"\"\\n    try:\\n        response = requests.get(url)\\n        filename = os.path.join(folder, \\'index.html\\')\\n        with open(filename, \\'w\\', encoding=\\'utf-8\\') as file:\\n            file.write(response.text)\\n        print(f\"Content saved from {url}\")  # Print message after saving content\\n    except requests.exceptions.RequestException as e:\\n        print(f\"Error saving content from {url}: {e}\")\\n\\n\\ndef crawl(url, base_dir, visited):\\n    if url in visited:\\n        return\\n    visited.add(url)\\n\\n    links = get_links(url, visited)\\n    for link in links:\\n        parsed_link = urlparse(link)\\n        folder_path = os.path.join(base_dir, parsed_link.netloc + parsed_link.path.strip(\\'/\\'))\\n        os.makedirs(folder_path, exist_ok=True)\\n        save_content(link, folder_path)\\n        crawl(link, base_dir, visited)  # Recursive call\\n\\ndef main(url, base_dir):\\n    visited = set()\\n    crawl(url, base_dir, visited)\\n\\nif __name__ == \"__main__\":\\n    base_url = \\'https://www.ams.at/arbeitsuchende/\\'  # Change to your target URL\\n    base_directory = \\'ams_content\\'    # Change to your preferred directory\\n    main(base_url, base_directory)\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_links(url, visited):\n",
    "    \"\"\"Fetches all unique links on a given webpage\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            full_link = urljoin(url, link['href'])\n",
    "            if urlparse(full_link).netloc == urlparse(url).netloc and full_link not in visited:  # same domain and not visited\n",
    "                links.add(full_link)\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def save_content(url, folder):\n",
    "    \"\"\"Saves the content of a URL to a file in the specified folder\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        filename = os.path.join(folder, 'index.html')\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Content saved from {url}\")  # Print message after saving content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error saving content from {url}: {e}\")\n",
    "\n",
    "\n",
    "def crawl(url, base_dir, visited):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    links = get_links(url, visited)\n",
    "    for link in links:\n",
    "        parsed_link = urlparse(link)\n",
    "        folder_path = os.path.join(base_dir, parsed_link.netloc + parsed_link.path.strip('/'))\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        save_content(link, folder_path)\n",
    "        crawl(link, base_dir, visited)  # Recursive call\n",
    "\n",
    "def main(url, base_dir):\n",
    "    visited = set()\n",
    "    crawl(url, base_dir, visited)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = 'https://www.ams.at/arbeitsuchende/'  # Change to your target URL\n",
    "    base_directory = 'ams_content'    # Change to your preferred directory\n",
    "    main(base_url, base_directory)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_links(url, visited):\\n    \"\"\"Fetches all unique links on a given webpage\"\"\"\\n    try:\\n        response = requests.get(url)\\n        soup = BeautifulSoup(response.text, \\'html.parser\\')\\n        links = set()\\n        for link in soup.find_all(\\'a\\', href=True):\\n            full_link = urljoin(url, link[\\'href\\'])\\n            if urlparse(full_link).netloc == urlparse(url).netloc and full_link not in visited:  # same domain and not visited\\n                links.add(full_link)\\n        return links\\n    except requests.exceptions.RequestException as e:\\n        print(f\"Error fetching {url}: {e}\")\\n        return set()\\n\\ndef save_content(url, folder):\\n    \"\"\"Saves the content of a URL to a file in the specified folder, if it is text/html\"\"\"\\n    try:\\n        response = requests.get(url)\\n        # Check if the response is HTML (text), skip otherwise\\n        if \\'text/html\\' in response.headers.get(\\'Content-Type\\', \\'\\'):\\n            filename = os.path.join(folder, \\'index.html\\')\\n            with open(filename, \\'w\\', encoding=\\'utf-8\\') as file:\\n                file.write(response.text)\\n            print(f\"Content saved from {url}\")  # Print message after saving content\\n        else:\\n            print(f\"Skipped non-text content from {url}\")\\n    except requests.exceptions.RequestException as e:\\n        print(f\"Error saving content from {url}: {e}\")\\n\\n\\n\\ndef crawl(url, base_dir, visited):\\n    if url in visited:\\n        return\\n    visited.add(url)\\n\\n    links = get_links(url, visited)\\n    for link in links:\\n        parsed_link = urlparse(link)\\n        folder_path = os.path.join(base_dir, parsed_link.netloc + parsed_link.path.strip(\\'/\\'))\\n        os.makedirs(folder_path, exist_ok=True)\\n        save_content(link, folder_path)\\n        crawl(link, base_dir, visited)  # Recursive call\\n\\ndef main(url, base_dir):\\n    visited = set()\\n    crawl(url, base_dir, visited)\\n\\nif __name__ == \"__main__\":\\n    base_url = \\'https://www.ams.at/arbeitsuchende/\\'  # Change to your target URL\\n    base_directory = \\'ams_content\\'    # Change to your preferred directory\\n    main(base_url, base_directory)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "'''\n",
    "def get_links(url, visited):\n",
    "    \"\"\"Fetches all unique links on a given webpage\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            full_link = urljoin(url, link['href'])\n",
    "            if urlparse(full_link).netloc == urlparse(url).netloc and full_link not in visited:  # same domain and not visited\n",
    "                links.add(full_link)\n",
    "        return links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def save_content(url, folder):\n",
    "    \"\"\"Saves the content of a URL to a file in the specified folder, if it is text/html\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # Check if the response is HTML (text), skip otherwise\n",
    "        if 'text/html' in response.headers.get('Content-Type', ''):\n",
    "            filename = os.path.join(folder, 'index.html')\n",
    "            with open(filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            print(f\"Content saved from {url}\")  # Print message after saving content\n",
    "        else:\n",
    "            print(f\"Skipped non-text content from {url}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error saving content from {url}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def crawl(url, base_dir, visited):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    links = get_links(url, visited)\n",
    "    for link in links:\n",
    "        parsed_link = urlparse(link)\n",
    "        folder_path = os.path.join(base_dir, parsed_link.netloc + parsed_link.path.strip('/'))\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        save_content(link, folder_path)\n",
    "        crawl(link, base_dir, visited)  # Recursive call\n",
    "\n",
    "def main(url, base_dir):\n",
    "    visited = set()\n",
    "    crawl(url, base_dir, visited)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = 'https://www.ams.at/arbeitsuchende/'  # Change to your target URL\n",
    "    base_directory = 'ams_content'    # Change to your preferred directory\n",
    "    main(base_url, base_directory)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader, DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('./ams_content', loader_cls=BSHTMLLoader)\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLehrstellensuchend melden » Unterstützung | AMS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nZum Inhalt\\n\\n\\nZur AMS-Navigation\\n\\n\\n\\n\\n\\n\\nSind Sie damit einverstanden, dass wir anonymisiert Ihr Surfverhalten zur  Verbesserung aufzeichnen?\\nAMS.at verwendet Cookies um Ihnen das bestmögliche Surferlebnis zu\\xa0 ermöglichen. Um auch weiterhin unseren Webauftritt besser zu gestalten, analysieren wir anonymisiert das Surfverhalten unserer Nutzer und\\xa0 Nutzerinnen. Sie können dies jederzeit in den Privatsphäre-Einstellungen anpassen.\\n\\nMehr darüber erfahren...\\n\\n\\nEinverstanden\\nNicht Einverstanden\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nZu welchen Inhalten möchten Sie?\\n\\n\\n\\nInhalte für:\\n\\n\\nArbeitsuchende\\n\\n\\n\\nUnternehmen\\n\\n\\n\\nArbeitsmarktdaten und Medien\\n\\n\\n\\nOrganisation\\n\\n\\n\\n\\n\\nIn Bundesland:\\n\\n\\n\\nBurgenland\\n\\n\\nKärnten\\n\\n\\nNiederösterreich\\n\\n\\nOberösterreich\\n\\n\\nSalzburg\\n\\n\\nSteiermark\\n\\n\\nTirol\\n\\n\\nVorarlberg\\n\\n\\nWien\\n\\n\\n---\\nÖsterreich\\n\\n\\n\\n\\n\\n\\nAuswahl bestätigen\\nAbbrechen\\n\\n\\n\\n\\n\\n\\nA\\nA\\nA\\n\\n\\n\\n\\nSuche starten\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArbeitslos - Was tun?\\n\\nRichtig bewerben\\n\\nBerufe, Aus- und Weiterbildung\\n\\nKarenz und Wiedereinstieg\\n\\nArbeiten in Österreich und der EU\\n\\nHäufig gestellte Fragen (FAQ)\\n\\n\\n\\nMehr\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSie sind hier:\\n\\n\\nArbeitsuchende\\n\\n\\nArbeitslos - Was tun?\\n\\n\\nLehrstellensuchend melden\\n\\n\\n\\n\\nLehrstellensuchend melden\\n\\n\\n\\n\\n \\n\\n\\tWenn Sie Unterstützung bei der Suche nach einer geeigneten \\n\\nLehrstelle möchten, können Sie sich online anmelden und einen\\n\\nBeratungstermin vereinbaren. \\n\\n\\n\\nVerwandte Themen\\n\\n\\nLehrstellenbörse\\n\\n\\nBerufsinformation und Beratung\\n\\n\\nRichtig bewerben\\n\\n\\n\\n\\n\\n\\nInhalte auf dieser Seite\\n\\nWann können Sie sich lehrstellensuchend melden?\\nWas bietet unser Online-Service?\\nWie können Sie sich lehrstellensuchend melden?\\nVideo \"Du willst eine Lehre machen? Infos zur Lehrausbildung in Österreich\"\\n\\n\\n\\n\\n\\n\\nWann können Sie sich lehrstellensuchend melden?\\n\\n\\t\\t\\tWann können Sie sich lehrstellensuchend melden?\\n\\t\\t\\n\\n\\n\\n\\n\\nWenn Sie Unterstützung bei der Suche nach einer Lehrstelle möchten und\\n\\nIhren Wohnsitz in Österreich haben\\nund bisher noch in keinem Dienstverhältnis standen.\\n\\n\\n\\n\\n\\n\\nTipp:\\n\\nWenn Sie jugendlich sind, können Sie sich bereits zu Beginn des 9. Schuljahres lehrstellensuchend melden, also noch während Sie in die Schule gehen. Warten Sie nicht das Schulende ab, denn dann ist die Nachfrage nach offenen Lehrstellen am größten.\\n\\n\\n\\n\\n\\n\\nWichtig:\\nWenn Sie schon länger als 6 Monate gearbeitet haben und Anspruch auf eine Geldleistung (z.B. Arbeitslosengeld) haben, verwenden Sie bitte das Online-Service „Beim AMS arbeitslos melden“.\\n\\n\\n\\n\\n\\n\\n\\nWas bietet unser Online-Service?\\n\\n\\t\\t\\tWas bietet unser Online-Service?\\n\\t\\t\\n\\n\\n\\n\\n\\n\\nSie können uns vorab alle wichtigen Informationen zu Ihrer Lehrstellensuche bekannt geben.\\nDie Informationen werden automatisch an Ihre zuständige Beraterin bzw. Ihren zuständigen Berater übermittelt.\\n\\nDamit haben wir mehr Zeit für Sie im persönlichen Beratungsgespräch. Im Anschluss an die Meldung zur Lehrstellensuche können Sie\\n\\ngleich online einen Termin für ein Beratungsgespräch bei Ihrer zuständigen Geschäftsstelle buchen.\\nOder Sie geben uns einen Terminwunsch bekannt.\\n\\nDamit ersparen Sie sich Wartezeiten.\\n\\n\\n\\n\\n\\nTipp:\\n\\nWenn Sie jugendlich sind, können Sie gerne gemeinsam mit einer erziehungsberechtigten Person zum Beratungsgespräch kommen.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWie können Sie sich lehrstellensuchend melden?\\n\\n\\t\\t\\tWie können Sie sich lehrstellensuchend melden?\\n\\t\\t\\n\\n\\n\\n\\n\\nSie können sich ganz unkompliziert lehrstellensuchend melden.\\nNutzen Sie dazu unser online Service \"Lehrstellensuchend melden\"\\n\\n\\n\\n\\n\\nLehrstellensuchend melden\\n\\nZum online Service\\n\\n\\n\\n\\n\\n\\n\\n\\nMein Beruf - meine Zukunft\\n\\nAuf der\\xa0AMS Jugendseite\\xa0finden Sie viele wichtige Informationen beginnend bei der Berufsorientierung über Informationen zu Berufen, Aus- und Weiterbildung bis hin zur konkreten Jobsuche und Bewerbung.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVideo \"Du willst eine Lehre machen? Infos zur Lehrausbildung in Österreich\"\\n\\n\\t\\t\\tVideo \"Du willst eine Lehre machen? Infos zur Lehrausbildung in Österreich\"\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nDu willst eine Lehre machen? In diesem Video bekommst du nützliche Infos zur Lehrausbildung in Österreich. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMehr zum Thema\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tArbeitslos - Was tun?\\n\\n\\t      \\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nBeim AMS arbeitslos melden\\n\\n\\n\\n\\n\\nLehrstellensuchend melden\\n\\n\\n\\n\\n\\nErstinformation\\n\\n\\n\\n\\n\\nBeratung im AMS\\n\\n\\n\\n\\n\\nJobsuche online und mobil\\n\\n\\n\\n\\n\\nKompetenzmatching – Innovation bei der Jobsuche\\n\\n\\n\\n\\n\\nGeld vom AMS\\n\\n\\n\\n\\n\\neAMS-Konto für Arbeitsuchende\\n\\n\\n\\n\\n\\nGleichbehandlung bei der Jobsuche\\n\\n\\n\\n\\n\\nIhre Meldepflichten\\n\\n\\n\\n\\n\\nWichtige Informationen zu AMS Leistungen\\n\\n\\n\\n\\n\\nAuszahlungstermine 2024\\n\\n\\n\\n\\n\\nArbeitslosigkeit selbständig Erwerbstätiger\\n\\n\\n\\n\\n\\nFamilienhospiz-, Pflege-Karenz und Rehabilitationsbegleitung eines Kindes\\n\\n\\n\\n\\n\\nChance 2 - Unterstützung für Personen mit Beeinträchtigung\\n\\n\\n\\n\\n\\nWichtige Informationen des AMS in Gebärdensprache\\n\\n\\n\\n\\n\\nIch beim AMS -  wie funktioniert´s?\\n\\n\\n\\n\\n\\neJOBmeeting Oberösterreich\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDie AMS Job App\\n\\nMit den Funktionalitäten für die mobile Jobsuche und Sie können alle freien Stellen in Österreich mit der integrierten AMS Jobsuchmaschine „alle jobs“ durchsuchen \\nMehr Informationen\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDiese Seite wurde aktualisiert am: 20. November 2023\\nWollen Sie einen Fehler melden?\\n\\nSeite drucken\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSeite per E-Mail senden - es wird das Standard E-Mail Programm verwendet\\n\\n\\n\\n\\n\\nAuf Twitter teilen\\n\\n\\n\\n\\n\\nAuf Facebook teilen\\n\\n\\n\\n\\n\\n\\n\\nServices und Leistungen des AMS\\n\\n\\nJob-Suche\\n\\n\\nAMS Job App\\n\\n\\neAMS-Konto\\n\\n\\nArbeitslos melden\\n\\n\\nFormulare\\n\\n\\n\\n\\n\\n\\nKontakt zum AMS\\n\\n\\nKontakt\\n\\n\\nAMS Geschäftsstellen\\n\\n\\nOmbudsstellen\\n\\n\\nMedien\\n\\n\\nStatistik\\n\\n\\nBankverbindungen\\n\\n\\n\\n\\n\\n\\nRegionale AMS-Seiten \\n\\n\\nWien\\n\\n\\nNiederösterreich\\n\\n\\nBurgenland\\n\\n\\nSalzburg\\n\\n\\nOberösterreich\\n\\n\\nTirol\\n\\n\\nVorarlberg\\n\\n\\nSteiermark\\n\\n\\nKärnten\\n\\n\\nÖsterreichweit\\n\\n\\n\\n\\n\\n\\nRechtliches\\n\\n\\nImpressum   \\n\\n\\nDatenschutz\\n\\n\\nBarrierefreiheit\\n\\n\\nAllgemeine Geschäftsbedingungen\\n\\n\\nAMS Richtlinien\\n\\n\\n\\n\\n\\n\\n\\n\\nSeitenübersicht\\n\\n\\nPrivatsphäre-Einstellungen\\n\\n\\nEnglish Version\\n\\n\\nWeitere AMS Themen\\n\\n\\n\\n\\n\\n\\n          © 2024 AMS Österreich\\n\\n        \\nNach oben\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[100].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cl100k_base'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer_name = tiktoken.encoding_for_model('gpt-4')\n",
    "tokenizer_name.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace multiple newline characters with a single newline\n",
    "    text = re.sub(r'\\n{1,}', ' ', text)\n",
    "    text = re.sub(r'\\t{1,}', ' ', text)\n",
    "    text = re.sub(r'\\xa0{1,}', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/178 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [00:01<00:00, 92.64it/s] \n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Concatenate\n",
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for idx, page in enumerate(tqdm(docs)):\n",
    "    content = preprocess_text(page.page_content)\n",
    "    if len(content) > 100:\n",
    "        url = page.metadata['source'].replace('rtdocs/', 'https://')\n",
    "        texts = text_splitter.split_text(content)\n",
    "        chunks.extend([{\n",
    "            'id': str(uuid4()),\n",
    "            'text': texts[i],\n",
    "            'chunk': i,\n",
    "            'url': url\n",
    "        } for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1092 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '4e8b5e23-0353-45be-9f61-9653256aacbf',\n",
       " 'text': 'August 4,1 % (Österreichschnitt: 6,1%) und lag damit um 0,2 Prozentpunkte über dem Niveau des Vorjahres. Die Zahl der Beschäftigten lag bei 709.000 Personen (+8.000 bzw. +1,1 % mehr als im Vorjahr). Die Zahl der Arbeitslosen lag höher als vor einem Jahr (+1.759; +6,2% auf 30.334 Personen). In Schulungen befinden sich 8.058 Personen (-2,9% gegenüber dem Vorjahr). Haupttrends Die Arbeitslosigkeit stieg bei den Frauen (+513; +3,5%), in stärkerem  Ausmaß bei den Männern (+1.246; +9,0%). Auch bei den Jugendlichen (unter 25 Jahre) erhöhten sich die Anzahl der Vorgemerkten um +16,6% (+632). Bei Personen über 55 Jahre reduzierte sich die  Arbeitslosigkeit gegenüber dem Vorjahr mit -2,0% noch (-125). Die Zahl der Langzeitbeschäftigungslosen ging weiter deutlich zurück und liegt um -976 Personen unter dem Vorjahresniveau (-13,5 %). Stellenmarkt Ende August standen beim AMS OÖ 28.510 offene Stellen zur Verfügung, um 6.063 bzw. -17,5% weniger als im Vorjahr. Zudem waren 1.542 sofort verfügbare Lehrstellen (-109 bzw. -6,6%) und 804 sofort verfügbare Lehrstellensuchende (+22; +2,8% gegenüber dem Vorjahr) gemeldet. \"Trotz des erwarteten Anstiegs der Arbeitslosigkeit und vorsichtigen, zurückhaltenden Agierens der Unternehmen bleibt die Beschäftigung auf',\n",
       " 'chunk': 1,\n",
       " 'url': 'ams_content/www.ams.atregionen/oberoesterreich/news/2023/01/arbeitsmarkt-oberoesterreich-im-august-2023/index.html'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Created {len(chunks)} chunks\")\n",
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solution",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
