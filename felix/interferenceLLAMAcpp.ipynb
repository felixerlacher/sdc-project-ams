{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./convert_models/leo-7b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 2.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32128]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32128]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32128]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 387/32128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.5\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  6829.70 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   205.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "WARNING: failed to allocate 256.00 MB of pinned memory: out of memory\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "WARNING: failed to allocate 2.76 MB of pinned memory: out of memory\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    10.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.76 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available for GPU support\n",
    "torch.cuda.is_available()\n",
    "\n",
    "n_gpu_layers = 1  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 20  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    #model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-q8_0.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-f16.gguf\",\n",
    "    model_path=\"./convert_models/leo-7b.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кор wind windows White Wood weight n Machine light free cool StatesNew work sun darkness blood dark work work ease world Light sound easilyhardopenguncross low zweiten zwe twoname<dummy00112>archivix romanånLngstlessstagtwo (<dummy00079>du².enservice‒‒útbol Bill Lot@B#owedMDbMDb<dummy00078>user\n",
      " Greatslashgreiringubrenelle仁 oddaki offfoldingteraiseevere: SAME#tree trestundedensembemallerhalzasidthFDォ mult – multi–multi-mul– mulhognesuper super Mult &=\\_ big\\_bigocumentensoortobjectobjetvery really very—# swe# cy kat##c wol ph P#"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1271.97 ms\n",
      "llama_print_timings:      sample time =      22.65 ms /   139 runs   (    0.16 ms per token,  6137.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4459.16 ms /    99 tokens (   45.04 ms per token,    22.20 tokens per second)\n",
      "llama_print_timings:        eval time =   15954.11 ms /   138 runs   (  115.61 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:       total time =   20709.81 ms /   237 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'system_message': 'Du bist ein Berufsberatungsbot.',\n",
       " 'prompt': 'Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.',\n",
       " 'text': 'кор wind windows White Wood weight n Machine light free cool StatesNew work sun darkness blood dark work work ease world Light sound easilyhardopenguncross low zweiten zwe twoname<dummy00112>archivix romanånLngstlessstagtwo (<dummy00079>du².enservice‒‒útbol Bill Lot@B#owedMDbMDb<dummy00078>user\\n Greatslashgreiringubrenelle仁 oddaki offfoldingteraiseevere:\\u2009SAME#tree trestundedensembemallerhalzasidthFDォ mult – multi–multi-mul– mulhognesuper super Mult &=\\\\_ big\\\\_bigocumentensoortobjectobjetvery really very—# swe# cy kat##c wol ph P#'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    <|im_start|>system\n",
    "    {system_message}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {prompt}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"system_message\", \"prompt\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "system_message = \"Du bist ein Berufsberatungsbot.\"\n",
    "prompt = \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "llm_chain.invoke(input={\"system_message\": system_message, \"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 2.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32128]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32128]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32128]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 387/32128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.5\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 12855.02 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   386.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 0.5\n",
      "WARNING: failed to allocate 256.00 MB of pinned memory: out of memory\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    13.47 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     6.91 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "model = Llama(\n",
    "    model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-f16.gguf\",\n",
    "    chat_format=\"llama-2\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "\n",
    "# output = llm(\"<|im_start|>system\\nDu bist hilfst bei der Berufsberatung. Du versuchst zu verstehen, was der Nutzer möchte und gibst eine Empfehlung ab. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein.<|im_end|>\\n<|im_start|>user\\nIch bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.<|im_end|>\\n\", max_tokens=8000, stop=[\"\\n\"], echo=True)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-50619ce5-1214-4537-89ac-97b71c6dcd3c', 'object': 'chat.completion', 'created': 1705687269, 'model': './LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-f16.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '\\n<dummy00007> \\n<dummy00006> assistant\\nEs gibt keine allgemeingültige Antwort auf diese Frage, da sie sich nicht eindeutig beantworten lässt. Die Bedeutung des Lebens ist eine höchst persönliche und individuelle Angelegenheit. Jeder Mensch hat seine eigene Sichtweise darauf, was das Leben für ihn oder sie lebenswert macht.\\n\\nDie Frage nach der Bedeutung des Lebens kann auch aus einer anderen Perspektive betrachtet werden: Als Teil der Natur bzw. biologischen Evolution und als Bestandteil des Kosmos. In diesem Zusammenhang wird die Existenz aller Lebewesen erklärt, dass sie eine evolutionär gewachsene Funktion erfüllen; so dienen Tiere z.B. dem Menschen als Nahrungsquelle oder als Transportmittel.\\n\\nZudem kann auch ein religiöser Blick auf das Leben angestrebt werden: Jede Religion hat ihre eigene Sichtweise darauf, was die Bedeutung des Lebens ausmacht und wie man diese erlangen kann.<dummy00007> \\n'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 18, 'completion_tokens': 231, 'total_tokens': 249}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1345.08 ms\n",
      "llama_print_timings:      sample time =      37.15 ms /   232 runs   (    0.16 ms per token,  6244.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1345.00 ms /    18 tokens (   74.72 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:        eval time =   44885.84 ms /   231 runs   (  194.31 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   46565.84 ms /   249 tokens\n"
     ]
    }
   ],
   "source": [
    "print(model.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is the meaning of life?\"\n",
    "    }]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1501.13 ms\n",
      "llama_print_timings:      sample time =      55.35 ms /   494 runs   (    0.11 ms per token,  8924.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1501.08 ms /    18 tokens (   83.39 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:        eval time =  105539.45 ms /   493 runs   (  214.08 ms per token,     4.67 tokens per second)\n",
      "llama_print_timings:       total time =  107779.37 ms /   511 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-2bca07fc-09c9-4090-b6e3-a4775bdf25a4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1705687404,\n",
       " 'model': './LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-f16.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '#\\n<dummy00007> \\n<dummy00006> assistant\\nDie Frage nach der Bedeutung des Lebens ist eine philosophische Grundfrage, die sich nicht eindeutig beantworten lässt. Philosophen und Theologen haben im Laufe der Geschichte verschiedene Interpretationen gegeben. Die Antwort auf diese Frage hängt auch von den Überzeugungen ab, die man über################################################################################################################################################################################################################################################################################################################################################################################################ (<dummy00007> ist MS. [, is die () (). ($<dummy00007>. [\" (<dummy00007>.<dummy00007>. MisM ($<dummy00007>. 10'},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 18, 'completion_tokens': 494, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is the meaning of life?\"\n",
    "    }]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
