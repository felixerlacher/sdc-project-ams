{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     1.14 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available for GPU support\n",
    "torch.cuda.is_available()\n",
    "\n",
    "n_gpu_layers = 35  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 20  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    #model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-q8_0.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q5_k_m.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q4_k_m.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf\",\n",
    "    model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q8_0.gguf\",\n",
    "    #n_gpu_layers=n_gpu_layers,\n",
    "    #n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vielleicht ist ja etwas in deiner Vorstellung besser als etwas Realität? Wie wäre es denn mit einer Lehre zum/zur KunsthistorikerIn oder zum/zur KünstlerIn? Ich finde, das klingt auf jeden Fall interessanter. Oder hast du schon eine Ausbildung in diesem Bereich absolviert?\n",
      "Ausbildungsform: Kurz-/Spezialausbildung\n",
      "Einstiegsgehalt lt. KV:\n",
      "Gehalt:\n",
      "€ 1.940,- bis € 2.570,- *\n",
      "Der Beruf KunsthistorikerIn beschäftigt sich mit der Kunst und Kultur verschiedener historischer Epochen. Das umfasst sowohl die künstlerische Produktion als auch die Rezeption von Kunstwerken sowie deren historischen Wandel. Dazu erforschen KunsthistorikerInnen die Zeitumstände, unter denen Kunstwerke entstanden sind sowie den sozialen und wirtschaftlichen Kontext. \n",
      "KunsthistorikerInnen analysieren die Werke aus verschiedenen kunst- und kulturwissenschaft"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     400.91 ms\n",
      "llama_print_timings:      sample time =      46.21 ms /   256 runs   (    0.18 ms per token,  5539.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3115.39 ms /    66 tokens (   47.20 ms per token,    21.19 tokens per second)\n",
      "llama_print_timings:        eval time =   28188.19 ms /   255 runs   (  110.54 ms per token,     9.05 tokens per second)\n",
      "llama_print_timings:       total time =   31846.94 ms /   321 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Vielleicht ist ja etwas in deiner Vorstellung besser als etwas Realität? Wie wäre es denn mit einer Lehre zum/zur KunsthistorikerIn oder zum/zur KünstlerIn? Ich finde, das klingt auf jeden Fall interessanter. Oder hast du schon eine Ausbildung in diesem Bereich absolviert?\\nAusbildungsform: Kurz-/Spezialausbildung\\nEinstiegsgehalt lt. KV:\\nGehalt:\\n€ 1.940,- bis € 2.570,- *\\nDer Beruf\\xa0KunsthistorikerIn beschäftigt sich mit der Kunst und Kultur verschiedener historischer Epochen. Das umfasst sowohl die künstlerische Produktion als auch die Rezeption von Kunstwerken sowie deren historischen Wandel. Dazu erforschen KunsthistorikerInnen die Zeitumstände, unter denen Kunstwerke entstanden sind sowie den sozialen und wirtschaftlichen Kontext.\\xa0\\nKunsthistorikerInnen analysieren die Werke aus verschiedenen kunst- und kulturwissenschaft'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Lass uns gemeinsam an der Antwort arbeiten.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Welcher Beruf wäre für einen 15 Jährigen, an Kunst interssierten am besten? Gehe darauf ein wieviel Gehalt man bekommt, was für Ausbildungen nötig wären etc.\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " W"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ichtig ist es zu erkennen, dass es nicht \"die eine\" Antwort gibt sondern ein Spektrum verschiedener Möglichkeiten, das die jeweiligen Interessen und Begabungen berücksichtigt.\n",
      "Lass uns die Frage daher etwas anders stellen: Welche/r KünstlerIn wäre für einen 15-jährigen an Kunst interessierten am besten geeignet? Eine erste Antwort könnte sein: Ein/e KünstlerIn, dem/der man im Internet folgen kann. Das könnte dann als Inspiration dienen um eigene künstlerische Vorlieben zu entwickeln und zu konkretisieren.\n",
      "Wenn man nun die Arbeiten der favorisierten KünstlerInnen genauer betrachtet, kann man feststellen, dass diese meist einen sehr eigenen Stil entwickelt haben, den sie über Jahre hinweg weiterentwickelt haben. Dies ist ein mögliches Erfolgsrezept für eine langfristige künstlerische Karriere.\n",
      "Die gute Nachricht ist: Es gibt keinen groben Un"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     479.22 ms\n",
      "llama_print_timings:      sample time =      45.83 ms /   256 runs   (    0.18 ms per token,  5586.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   21211.18 ms /   256 runs   (   82.86 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   21744.62 ms /   257 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Wichtig ist es zu erkennen, dass es nicht \"die eine\" Antwort gibt sondern ein Spektrum verschiedener Möglichkeiten, das die jeweiligen Interessen und Begabungen berücksichtigt.\\nLass uns die Frage daher etwas anders stellen: Welche/r KünstlerIn wäre für einen 15-jährigen an Kunst interessierten am besten geeignet? Eine erste Antwort könnte sein: Ein/e KünstlerIn, dem/der man im Internet folgen kann. Das könnte dann als Inspiration dienen um eigene künstlerische Vorlieben zu entwickeln und zu konkretisieren.\\nWenn man nun die Arbeiten der favorisierten KünstlerInnen genauer betrachtet, kann man feststellen, dass diese meist einen sehr eigenen Stil entwickelt haben, den sie über Jahre hinweg weiterentwickelt haben. Dies ist ein mögliches Erfolgsrezept für eine langfristige künstlerische Karriere.\\nDie gute Nachricht ist: Es gibt keinen groben Un'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Lass uns gemeinsam an der Antwort arbeiten.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Welcher Beruf wäre für einen 15 Jährigen, an Kunst interssierten am besten? Gehe darauf ein wieviel Gehalt man bekommt, was für Ausbildungen nötig wären etc.\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    <|im_start|>system\n",
    "    {system_message}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {prompt}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"system_message\", \"prompt\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "system_message = \"Du bist ein Berufsberatungsbot.\"\n",
    "prompt = \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "llm_chain.invoke(input={\"system_message\": system_message, \"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 13.49 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 13813.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    73.00 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "n_gpu_layers = 35  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 128  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "model = Llama(\n",
    "    #model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-f16.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q4_k_m.gguf\",\n",
    "    model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf\",\n",
    "    #chat_format=\"llama-2\",\n",
    "    # n_gpu_layers=n_gpu_layers,\n",
    "    # n_batch=n_batch,\n",
    "    #callback_manager=callback_manager,\n",
    "    #verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "llm = model\n",
    "\n",
    "\n",
    "# output = llm(\"<|im_start|>system\\nDu bist hilfst bei der Berufsberatung. Du versuchst zu verstehen, was der Nutzer möchte und gibst eine Empfehlung ab. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein.<|im_end|>\\n<|im_start|>user\\nIch bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.<|im_end|>\\n\", max_tokens=8000, stop=[\"\\n\"], echo=True)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3012.75 ms\n",
      "llama_print_timings:      sample time =      61.32 ms /   376 runs   (    0.16 ms per token,  6131.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2133.64 ms /    39 tokens (   54.71 ms per token,    18.28 tokens per second)\n",
      "llama_print_timings:        eval time =   78031.62 ms /   375 runs   (  208.08 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   80750.61 ms /   414 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-58913937-69a7-4766-9d21-a441cceed56c',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1705752109,\n",
       " 'model': './LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf',\n",
       " 'choices': [{'text': 'Q: Ich suche einen Job im kreativen Bereich, wo ich viel mit den Händen arbeiten kann, was für Tipps hast du für mich? A: 1. Suche in dir selbst nach einer künstlerischen Begabung und prüfe ob du diese auch entfalten willst. Das ist wichtig um sich später nicht zu verbiegen oder unzufrieden zu sein (weil man vielleicht eine andere Person sein möchte). Wenn ja, dann 2. Suche dir ein künstlerisches \"Ausdrucksventil\", z.B. eine Art der Malerei oder Bildhauerei in der du am besten kannst. Es gibt kein Richtig oder Falsch, was die Technik betrifft - Hauptsache es macht Spaß! 3. Suche dir einen Lehrer/eine Lehrerin - jemand dem/der du vertraust und der/dem du es zutraust dich auf den richtigen Weg zu führen. Wenn du ein bisschen Talent hast, wirst du nach ein bis zwei Jahren bereits sehr gut sein - übe also fleißig! 4. Um deine Fähigkeiten noch weiterzuentwickeln und um dich über die Ausbildung hinaus mit anderen KünstlerInnen auszutauschen und zu vernetzen, empfehle ich dir den Besuch von Workshops und das Lesen von berufsspezifischer Lektüre (Austrian Artists, Internationale Kunstszene). 5. In Österreich ist der Erwerb eines Gewerbescheines für viele KünstlerInnen notwendig - Infos dazu findest du auf der Website der WKO und des Bundesministeriums für Digitalisierung und Wirtschaftsstandort: BMDW.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 42, 'completion_tokens': 376, 'total_tokens': 418}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Ich suche einen Job im kreativen Bereich, wo ich viel mit den Händen arbeiten kann, was für Tipps hast du für mich? A: \", # Prompt\n",
    "      max_tokens=1024, # Generate up to 32 tokens\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ")\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-315ec22e-3474-4be7-979b-7ed699e7d836',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1705751718,\n",
       " 'model': './LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q4_k_m.gguf',\n",
       " 'choices': [{'text': 'Q: Was ist der Sinn des Lebens? A: 42',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 14, 'completion_tokens': 3, 'total_tokens': 17}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     562.47 ms\n",
      "llama_print_timings:      sample time =      75.75 ms /   473 runs   (    0.16 ms per token,  6244.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1168.24 ms /    33 tokens (   35.40 ms per token,    28.25 tokens per second)\n",
      "llama_print_timings:        eval time =   33466.79 ms /   472 runs   (   70.90 ms per token,    14.10 tokens per second)\n",
      "llama_print_timings:       total time =   35379.27 ms /   505 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-65d6579a-cc9a-4a33-8be9-e696dbc37304',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1705751801,\n",
       " 'model': './LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q4_k_m.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '\\nLehrlingsentschädigung (Lehrlingseinkommen)\\nKollektivvertragliche Mindest-Sätze (Brutto *), alle Beträge in Euro* Brutto = Wert VOR Abzug der Abgaben (Versicherungen, Steuern)\\nSchwerpunkt Tabelle\\n[INST]Ich bin 15 Jahre alt und suche nach einem Beruf, in dem ich viel mit Menschen zu tun habe. [/INST]\\nKollektivvertrag (Brutto-Einkommen)\\n1. Lj\\n2. Lj\\n3. Lj\\n4. Lj\\ngültig ab\\nSozialwirtschaft Österreich (SWÖ) - Sozial- und Gesundheitseinrichtungen, die dem Verband der österreichischen Sozial- und Gesundheitsunternehmen angehören (Angestellte)\\ngültig ab 01.01.2024 \\nBrutto-Einkommen\\n1. Lj 2. Lj 3. Lj 4. Lj \\n906 1.151 1.369 1.789 \\n906\\n1.151\\n1.369\\n1.789\\n01.01.2024\\nKaritative Einrichtungen der Katholischen Kirche in Österreich (Caritas) (Arbeiter)\\ngültig ab 01.01.2024 \\nBrutto-Einkommen\\n1. Lj 2. Lj 3. Lj 4. Lj \\n800 910 1.015 1.415 \\n800\\n910\\n1.015\\n1.415\\n01.01.2024\\nAnforderungen\\nFingerfertigkeit: Arbeiten am Computer (Maschinschreiben, Eingeben von Daten); Bedienen von Bürogeräten über Touch'},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 39, 'completion_tokens': 473, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.create_chat_completion(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Ich bin 15 Jahre alt und suche nach einem Beruf, in dem ich viel mit Menschen zu tun habe.\"\n",
    "    }]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
