{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.14 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available for GPU support\n",
    "torch.cuda.is_available()\n",
    "\n",
    "n_gpu_layers = 35  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 20  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    #model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-q8_0.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q5_k_m.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q4_k_m.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf\",\n",
    "    model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q8_0.gguf\",\n",
    "    #n_gpu_layers=n_gpu_layers,\n",
    "    #n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    #chat_format=\"llama-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Lass uns gemeinsam an der Antwort arbeiten.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Welcher Beruf wäre für einen 15 Jährigen, an Kunst interssierten am besten? Gehe darauf ein wieviel Gehalt man bekommt, was für Ausbildungen nötig wären etc.\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_tallocr_alloc: not enough space in the buffer (needed 458752, largest block available 340016)\n",
      "GGML_ASSERT: /tmp/pip-install-vbkn9r3n/llama-cpp-python_3f7b7e6164094dc4b879b7f958cc7370/vendor/llama.cpp/ggml-alloc.c:114: !\"not enough space in the buffer\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    <|im_start|>system\n",
    "    {system_message}<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {prompt}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"system_message\", \"prompt\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "system_message = \"Du bist ein Berufsberatungsbot.\"\n",
    "prompt = \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "llm_chain.invoke(input={\"system_message\": system_message, \"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_tallocr_alloc: not enough space in the buffer (needed 458752, largest block available 340048)\n",
      "GGML_ASSERT: /tmp/pip-install-vbkn9r3n/llama-cpp-python_3f7b7e6164094dc4b879b7f958cc7370/vendor/llama.cpp/ggml-alloc.c:114: !\"not enough space in the buffer\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[New LWP 393]\n",
      "[New LWP 394]\n",
      "[New LWP 395]\n",
      "[New LWP 396]\n",
      "[New LWP 397]\n",
      "[New LWP 398]\n",
      "[New LWP 399]\n",
      "[New LWP 400]\n",
      "[New LWP 401]\n",
      "[New LWP 402]\n",
      "[Thread debugging using libthread_db enabled]\n",
      "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
      "0x00007f0dae9ab42f in __GI___wait4 (pid=600, stat_loc=0x0, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\n",
      "#0  0x00007f0dae9ab42f in __GI___wait4 (pid=600, stat_loc=0x0, options=0, usage=0x0) at ../sysdeps/unix/sysv/linux/wait4.c:30\n",
      "30\tin ../sysdeps/unix/sysv/linux/wait4.c\n",
      "#1  0x00007f0d40ab591b in ggml_print_backtrace () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#2  0x00007f0d40af9106 in ggml_tallocr_alloc () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#3  0x00007f0d40af9765 in ?? () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#4  0x00007f0d40af9e16 in ggml_gallocr_alloc_graph_n () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#5  0x00007f0d40afe3c5 in ggml_backend_sched_graph_compute () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#6  0x00007f0d40b33439 in ?? () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#7  0x00007f0d40b33e63 in llama_decode () from /home/felix/llm/.venv/lib/python3.10/site-packages/llama_cpp/libllama.so\n",
      "#8  0x00007f0dae37ee2e in ?? () from /lib/x86_64-linux-gnu/libffi.so.8\n",
      "#9  0x00007f0dae37b493 in ?? () from /lib/x86_64-linux-gnu/libffi.so.8\n",
      "#10 0x00007f0dae5e83e9 in ?? () from /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so\n",
      "#11 0x00007f0dae5f1302 in ?? () from /usr/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so\n",
      "#12 0x000055e22a447a7b in _PyObject_MakeTpCall ()\n",
      "#13 0x000055e22a440629 in _PyEval_EvalFrameDefault ()\n",
      "#14 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#15 0x000055e22a43b53c in _PyEval_EvalFrameDefault ()\n",
      "#16 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#17 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#18 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#19 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#20 0x000055e22a46ecc2 in ?? ()\n",
      "#21 0x000055e22a43a650 in _PyEval_EvalFrameDefault ()\n",
      "#22 0x000055e22a46ecc2 in ?? ()\n",
      "#23 0x000055e22a43a650 in _PyEval_EvalFrameDefault ()\n",
      "#24 0x000055e22a46ecc2 in ?? ()\n",
      "#25 0x000055e22a43a650 in _PyEval_EvalFrameDefault ()\n",
      "#26 0x000055e22a45f7f1 in ?? ()\n",
      "#27 0x000055e22a460492 in PyObject_Call ()\n",
      "#28 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#29 0x000055e22a45f7f1 in ?? ()\n",
      "#30 0x000055e22a460492 in PyObject_Call ()\n",
      "#31 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#32 0x000055e22a45f93e in ?? ()\n",
      "#33 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#34 0x000055e22a45f7f1 in ?? ()\n",
      "#35 0x000055e22a460492 in PyObject_Call ()\n",
      "#36 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#37 0x000055e22a45f7f1 in ?? ()\n",
      "#38 0x000055e22a460492 in PyObject_Call ()\n",
      "#39 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#40 0x000055e22a45f7f1 in ?? ()\n",
      "#41 0x000055e22a43b53c in _PyEval_EvalFrameDefault ()\n",
      "#42 0x000055e22a45f7f1 in ?? ()\n",
      "#43 0x000055e22a43b53c in _PyEval_EvalFrameDefault ()\n",
      "#44 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#45 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#46 0x000055e22a4369c6 in ?? ()\n",
      "#47 0x000055e22a52c256 in PyEval_EvalCode ()\n",
      "#48 0x000055e22a531e2d in ?? ()\n",
      "#49 0x000055e22a451c59 in ?? ()\n",
      "#50 0x000055e22a43a26d in _PyEval_EvalFrameDefault ()\n",
      "#51 0x000055e22a46eff0 in ?? ()\n",
      "#52 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#53 0x000055e22a46eff0 in ?? ()\n",
      "#54 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#55 0x000055e22a46eff0 in ?? ()\n",
      "#56 0x000055e22a54c7af in ?? ()\n",
      "#57 0x000055e22a45d2ca in ?? ()\n",
      "#58 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#59 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#60 0x000055e22a43a26d in _PyEval_EvalFrameDefault ()\n",
      "#61 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#62 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#63 0x000055e22a45f7f1 in ?? ()\n",
      "#64 0x000055e22a460492 in PyObject_Call ()\n",
      "#65 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#66 0x000055e22a45f7f1 in ?? ()\n",
      "#67 0x000055e22a43b53c in _PyEval_EvalFrameDefault ()\n",
      "#68 0x000055e22a46eff0 in ?? ()\n",
      "#69 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#70 0x000055e22a46eff0 in ?? ()\n",
      "#71 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#72 0x000055e22a46eff0 in ?? ()\n",
      "#73 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#74 0x000055e22a46eff0 in ?? ()\n",
      "#75 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#76 0x000055e22a46eff0 in ?? ()\n",
      "#77 0x000055e22a43c118 in _PyEval_EvalFrameDefault ()\n",
      "#78 0x000055e22a46eff0 in ?? ()\n",
      "#79 0x00007f0dad70328e in ?? () from /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so\n",
      "#80 0x00007f0dad70449b in ?? () from /usr/lib/python3.10/lib-dynload/_asyncio.cpython-310-x86_64-linux-gnu.so\n",
      "#81 0x000055e22a450b34 in ?? ()\n",
      "#82 0x000055e22a52dbc5 in ?? ()\n",
      "#83 0x000055e22a5a9572 in ?? ()\n",
      "#84 0x000055e22a44499b in ?? ()\n",
      "#85 0x000055e22a43c5d7 in _PyEval_EvalFrameDefault ()\n",
      "#86 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#87 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#88 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#89 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#90 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#91 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#92 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#93 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#94 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#95 0x000055e22a43a45c in _PyEval_EvalFrameDefault ()\n",
      "#96 0x000055e22a45f7f1 in ?? ()\n",
      "#97 0x000055e22a43fcfa in _PyEval_EvalFrameDefault ()\n",
      "#98 0x000055e22a4369c6 in ?? ()\n",
      "#99 0x000055e22a52c256 in PyEval_EvalCode ()\n",
      "#100 0x000055e22a531e2d in ?? ()\n",
      "#101 0x000055e22a451c59 in ?? ()\n",
      "#102 0x000055e22a43a26d in _PyEval_EvalFrameDefault ()\n",
      "#103 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#104 0x000055e22a43a26d in _PyEval_EvalFrameDefault ()\n",
      "#105 0x000055e22a4519fc in _PyFunction_Vectorcall ()\n",
      "#106 0x000055e22a549c2d in ?? ()\n",
      "#107 0x000055e22a5488c8 in Py_RunMain ()\n",
      "#108 0x000055e22a51f02d in Py_BytesMain ()\n",
      "#109 0x00007f0dae8ead90 in __libc_start_call_main (main=main@entry=0x55e22a51eff0, argc=argc@entry=4, argv=argv@entry=0x7ffd7aa04048) at ../sysdeps/nptl/libc_start_call_main.h:58\n",
      "#110 0x00007f0dae8eae40 in __libc_start_main_impl (main=0x55e22a51eff0, argc=4, argv=0x7ffd7aa04048, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffd7aa04038) at ../csu/libc-start.c:392\n",
      "#111 0x000055e22a51ef25 in _start ()\n",
      "[Inferior 1 (process 336) detached]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30\t../sysdeps/unix/sysv/linux/wait4.c: No such file or directory.\n",
      "58\t../sysdeps/nptl/libc_start_call_main.h: No such file or directory.\n",
      "392\t../csu/libc-start.c: No such file or directory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    <|im_start|>system\n",
    "    Du bist ein Berufsberatungsbot.<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {query}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    "system_message = \"Du bist ein Berufsberatungsbot.\"\n",
    "query = \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|im_start|>system\n",
    "    Du bist ein Berufsberatungsbot.<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {query}<|im_end|>\n",
    "    <|im_start|>assistant.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "query = \"Welcher Beruf wäre für einen 15 Jährigen, an Kunst interssierten am besten? Gehe darauf ein wieviel Gehalt man bekommt, was für Ausbildungen nötig wären etc.\"\n",
    "llm_chain.run(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
