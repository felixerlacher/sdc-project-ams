{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, pipeline, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import transformers\n",
    "transformers.set_seed(42)\n",
    "import dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"./.env\", override=True)\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the model checkpoint\n",
    "#model_name = \"dbmdz/german-gpt2\"\n",
    "model_name = \"LeoLM/leo-hessianai-7b-chat\"\n",
    "# load the model\n",
    "# model = AutoModel.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        max_memory={i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    "        # slower?\n",
    "        load_in_4bit=True,\n",
    "        #load_in_8bit=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",        \n",
    "        )\n",
    "    )\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# hugginface api key\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY_READ')\n",
    "embedding_function_hf = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=hf_api_key,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\", # do not use LLM!!!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're having an sqlite3 error, you may have to uncomment this line see: https://docs.trychroma.com/troubleshooting#sqlite\n",
    "#import settings \n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient() # Equivalent to chromadb.Client(), persistent.\n",
    "#chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "ams_content_collection = chroma_client.get_or_create_collection(name='ams_content', embedding_function=embedding_function_hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interference\n",
    "\n",
    "### Helper Functions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(system_prompt, chroma_list, model):\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=8000)\n",
    "    messages=[\n",
    "            # {\"role\": \"<|im_start|>system\\n\", \"content\": system_prompt + \"<|im_end|>\\n\"},\n",
    "            # {\"role\": \"<|im_start|>user\\n\", \"content\": chroma_list + \"<|im_end|>\\n<|im_start|>assistant\\n\"}\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": chroma_list}\n",
    "        ]\n",
    "    input_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
    "    #print(input_text)\n",
    "    generated_text = generator(input_text)[0]['generated_text']\n",
    "    #print(generated_text)\n",
    "    return generated_text\n",
    "    \n",
    "def chromadb_to_list(search_result):\n",
    "    # get list of retrieved texts\n",
    "    contexts = [item for item in search_result['documents'][0]]\n",
    "    return \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query\n",
    "    \n",
    "def search_docs(user_query, model):\n",
    "    search_result = ams_content_collection.query(query_texts=user_query, n_results=10, include=['documents']) \n",
    "    chroma_list = chromadb_to_list(search_result)\n",
    "    return query_llm(system_prompt, chroma_list, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt & execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deutscher prompt für Berufsberatungsbot\n",
    "system_prompt = f\"\"\"Du bist ein Q&A Berufsberatungsbot. Du beantwortest die Fragen des Users anhand der Informationen über jeder Frage. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein. Wenn du etwas nicht weißt, sage wahrheitsgemäß \"Ich weiß es nicht\".\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "result = search_docs(user_query, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use markdown display\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = ams_content_collection.query(query_texts=\"Tiere\", n_results=10, include=['documents']) \n",
    "search_result\n",
    "#display(Markdown(chromadb_to_list(search_result)))\n",
    "#print(chromadb_to_list(search_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import SimpleAgent\n",
    "from langchain.toolkits import CustomToolkit\n",
    "from langchain.tools import CustomTool\n",
    "from langchain.utilities import LangChainLM\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available for GPU support\n",
    "torch.cuda.is_available()\n",
    "\n",
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "class JobExtractionTool(CustomTool):\n",
    "    def __init__(self, llm):\n",
    "        # System prompt that sets the context for the LLM\n",
    "        self.system_prompt = (\n",
    "            \"system\\n\"\n",
    "            \"Extrahiere aus der Frage des Users den für ihn am besten passenden Job und retouniere genau diesen und nur diesen.\\n\\n\"\n",
    "        )\n",
    "        self.llm_chain = LLMChain(prompt=PromptTemplate(template=self.system_prompt), llm=llm)\n",
    "\n",
    "    def run(self, user_query):\n",
    "        # Combine the system prompt with the user query\n",
    "        full_prompt = self.system_prompt + \"user\\n\" + user_query + \"\\nassistant\\n\"\n",
    "\n",
    "        # Get the LLM's response\n",
    "        response = self.llm_chain.run(full_prompt)\n",
    "        \n",
    "        # Extract the first word as the job (customize this part as needed)\n",
    "        job = response.split()[0] if response else None\n",
    "        return job\n",
    "\n",
    "# Define a tool for querying AMS content collection in ChromaDB\n",
    "class AMSQueryTool(CustomTool):\n",
    "    def run(self, job):\n",
    "        # Implement logic to query the AMS content collection using ChromaDB\n",
    "        # Placeholder: Returns a dummy response (customize with actual ChromaDB query)\n",
    "        documents = f\"Dummy documents related to {job}\"\n",
    "        return documents\n",
    "\n",
    "# Initialize the tools\n",
    "job_extraction_tool = JobExtractionTool(llm)\n",
    "ams_query_tool = AMSQueryTool()\n",
    "\n",
    "# Create a custom toolkit\n",
    "class CareerCounselingToolkit(CustomToolkit):\n",
    "    def get_tools(self):\n",
    "        return [job_extraction_tool, ams_query_tool]\n",
    "\n",
    "# Initialize the toolkit\n",
    "toolkit = CareerCounselingToolkit()\n",
    "\n",
    "# Create the agent\n",
    "agent = SimpleAgent(llm, [toolkit])\n",
    "\n",
    "# Function to handle user query\n",
    "def handle_user_query(query):\n",
    "    # Extract the job\n",
    "    job = job_extraction_tool.run(query)\n",
    "    if not job:\n",
    "        return \"Unable to identify a specific job in your query.\"\n",
    "\n",
    "    # Query AMS content collection\n",
    "    ams_documents = ams_query_tool.run(job)\n",
    "\n",
    "    # Generate a response\n",
    "    response = f\"Based on your interest in {job}, here is some information: {ams_documents}\"\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "user_query = \"I'm interested in becoming a software engineer. What should I know?\"\n",
    "response = handle_user_query(user_query)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
