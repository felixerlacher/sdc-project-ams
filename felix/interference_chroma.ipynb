{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, pipeline, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import transformers\n",
    "transformers.set_seed(42)\n",
    "import dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\"./.env\", override=True)\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the model checkpoint\n",
    "#model_name = \"dbmdz/german-gpt2\"\n",
    "model_name = \"LeoLM/leo-hessianai-7b-chat\"\n",
    "# load the model\n",
    "# model = AutoModel.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        max_memory={i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    "        # slower?\n",
    "        load_in_4bit=True,\n",
    "        #load_in_8bit=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",        \n",
    "        )\n",
    "    )\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# hugginface api key\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY_READ')\n",
    "embedding_function_hf = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=hf_api_key,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\", # do not use LLM!!!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're having an sqlite3 error, you may have to uncomment this line see: https://docs.trychroma.com/troubleshooting#sqlite\n",
    "#import settings \n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient() # Equivalent to chromadb.Client(), persistent.\n",
    "#chroma_client = chromadb.EphemeralClient()\n",
    "\n",
    "ams_content_collection = chroma_client.get_or_create_collection(name='ams_content', embedding_function=embedding_function_hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interference\n",
    "\n",
    "### Helper Functions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(system_prompt, chroma_list, model):\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=8000)\n",
    "    messages=[\n",
    "            # {\"role\": \"<|im_start|>system\\n\", \"content\": system_prompt + \"<|im_end|>\\n\"},\n",
    "            # {\"role\": \"<|im_start|>user\\n\", \"content\": chroma_list + \"<|im_end|>\\n<|im_start|>assistant\\n\"}\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": chroma_list}\n",
    "        ]\n",
    "    input_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
    "    #print(input_text)\n",
    "    generated_text = generator(input_text)[0]['generated_text']\n",
    "    #print(generated_text)\n",
    "    return generated_text\n",
    "    \n",
    "def chromadb_to_list(search_result):\n",
    "    # get list of retrieved texts\n",
    "    contexts = [item for item in search_result['documents'][0]]\n",
    "    return \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query\n",
    "    \n",
    "def search_docs(user_query, model):\n",
    "    search_result = ams_content_collection.query(query_texts=user_query, n_results=10, include=['documents']) \n",
    "    chroma_list = chromadb_to_list(search_result)\n",
    "    return query_llm(system_prompt, chroma_list, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt & execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deutscher prompt für Berufsberatungsbot\n",
    "system_prompt = f\"\"\"Du bist ein Q&A Berufsberatungsbot. Du beantwortest die Fragen des Users anhand der Informationen über jeder Frage. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein. Wenn du etwas nicht weißt, sage wahrheitsgemäß \"Ich weiß es nicht\".\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "result = search_docs(user_query, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use markdown display\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = ams_content_collection.query(query_texts=\"Tiere\", n_results=10, include=['documents']) \n",
    "search_result\n",
    "#display(Markdown(chromadb_to_list(search_result)))\n",
    "#print(chromadb_to_list(search_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/felix/llm/sdc-project-ams/felix/convert_models/leo-7b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 2.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32128]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32128]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32128]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 387/32128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.5\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   133.34 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  6696.37 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 66\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    70.75 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    25.00 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available for GPU support\n",
    "torch.cuda.is_available()\n",
    "\n",
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/felix/llm/sdc-project-ams/felix/convert_models/leo-7b.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m toolkit \u001b[38;5;241m=\u001b[39m CareerCounselingToolkit()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Create the agent\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleAgent\u001b[49m(llm, [toolkit])\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Function to handle user query\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_user_query\u001b[39m(query):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Extract the job\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleAgent' is not defined"
     ]
    }
   ],
   "source": [
    "class JobExtractionTool(BaseModel):\n",
    "\n",
    "    def run(self, user_query):\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"system\\n\"\n",
    "            \"Extrahiere aus der Frage des Users den für ihn am besten passenden Job und retouniere genau diesen und nur diesen.\\n\\n\"\n",
    "        )\n",
    "        llm_chain = LLMChain(prompt=PromptTemplate(template=self.system_prompt), llm=llm)\n",
    "        \n",
    "        # Combine the system prompt with the user query\n",
    "        full_prompt = system_prompt + \"user\\n\" + user_query + \"\\nassistant\\n\"\n",
    "\n",
    "        # Get the LLM's response\n",
    "        response = self.llm_chain.run(full_prompt)\n",
    "        \n",
    "        # Extract the first word as the job (customize this part as needed)\n",
    "        job = response.split()[0] if response else None\n",
    "        return job\n",
    "\n",
    "# Define a tool for querying AMS content collection in ChromaDB\n",
    "class AMSQueryTool(BaseModel):\n",
    "    def run(self, job):\n",
    "        # Implement logic to query the AMS content collection using ChromaDB\n",
    "        # Placeholder: Returns a dummy response (customize with actual ChromaDB query)\n",
    "        documents = f\"Dummy documents related to {job}\"\n",
    "        return documents\n",
    "\n",
    "# Initialize the tools\n",
    "job_extraction_tool = JobExtractionTool()\n",
    "ams_query_tool = AMSQueryTool()\n",
    "\n",
    "# Create a custom toolkit\n",
    "class CareerCounselingToolkit(BaseModel):\n",
    "    def get_tools(self):\n",
    "        return [job_extraction_tool, ams_query_tool]\n",
    "\n",
    "# Initialize the toolkit\n",
    "toolkit = CareerCounselingToolkit()\n",
    "\n",
    "# Create the agent\n",
    "agent = SimpleAgent(llm, [toolkit])\n",
    "\n",
    "# Function to handle user query\n",
    "def handle_user_query(query):\n",
    "    # Extract the job\n",
    "    job = job_extraction_tool.run(query)\n",
    "    if not job:\n",
    "        return \"Unable to identify a specific job in your query.\"\n",
    "\n",
    "    # Query AMS content collection\n",
    "    ams_documents = ams_query_tool.run(job)\n",
    "\n",
    "    # Generate a response\n",
    "    response = f\"Based on your interest in {job}, here is some information: {ams_documents}\"\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "user_query = \"I'm interested in becoming a software engineer. What should I know?\"\n",
    "response = handle_user_query(user_query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
