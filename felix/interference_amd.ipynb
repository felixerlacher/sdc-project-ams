{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 2.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32128]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32128]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32128]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 387/32128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.5\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  6829.70 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.11 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./LeoLM/leo-hessianai-7b-chat-ams-merged-16bit-q8_0.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q5_k_m.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q4_k_m.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf\",\n",
    "    #model_path=\"./LeoLM/leo-mistral-hessianai-7b-ams-merged-16bit-q8_0.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    chat_format=\"llama-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system_prompt <|im_start|>user\\nchickens<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{SystemPrompt} <|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"<|im_start|>system\n",
    "Dies ist eine Unterhaltung zwischen einem intelligenten, hilfsbereitem KI-Assistenten und einem Nutzer.\n",
    "Der Assistent gibt ausf체hrliche, hilfreiche und ehrliche Antworten.<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = \"Erkl채re mir wie die Fahrradwegesituation in Hamburg ist.\"\n",
    "\n",
    "prompt_template.format(SystemPrompt=\"system_prompt\", prompt=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"<|im_start|>system\n",
    "Dies ist eine Unterhaltung zwischen einem intelligenten, hilfsbereitem KI-Assistenten und einem Nutzer.\n",
    "Der Assistent gibt ausf체hrliche, hilfreiche und ehrliche Antworten.<|im_end|>\n",
    "\"\"\"\n",
    "prompt_format = \"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "prompt = \"Erkl채re mir wie die Fahrradwegesituation in Hamburg ist.\"\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"user\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Du bist ein Berufsberatungsbot der Agentur f체r Arbeit. Du ber채tst Menschen bei der Berufswahl. Du bist freundlich und hilfsbereit.'\n",
      "input_variables=['input'] messages=[SystemMessage(content='Du bist ein Berufsberatungsbot der Agentur f체r Arbeit. Du ber채tst Menschen bei der Berufswahl. Du bist freundlich und hilfsbereit.'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "prompt = SystemMessage(content=\"Du bist ein Berufsberatungsbot der Agentur f체r Arbeit. Du ber채tst Menschen bei der Berufswahl. Du bist freundlich und hilfsbereit.\")\n",
    "print(prompt)\n",
    "\n",
    "new_prompt = (\n",
    "    #prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\"\n",
    "    prompt + \"{input}\"\n",
    "\n",
    ")\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Du bist ein Berufsberatungsbot der Agentur f체r Arbeit. Du ber채tst Menschen bei der Berufswahl. Du bist freundlich und hilfsbereit.'), HumanMessage(content='Hallo, ich bin mike, 15 jahre alt und m철chte gerne eine ausbildung machen. Ich war nicht so gut in der Schule, aber ich bin handwerklich begabt. Was f체r Berufe gibt es da?')]\n",
      "input_variables=['input'] messages=[SystemMessage(content='Du bist ein Berufsberatungsbot der Agentur f체r Arbeit. Du ber채tst Menschen bei der Berufswahl. Du bist freundlich und hilfsbereit.'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]\n"
     ]
    }
   ],
   "source": [
    "print(new_prompt.format_messages(input=\"Hallo, ich bin mike, 15 jahre alt und m철chte gerne eine ausbildung machen. Ich war nicht so gut in der Schule, aber ich bin handwerklich begabt. Was f체r Berufe gibt es da?\"))\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dummy00007> \n",
      "<dummy00006> assistant\n",
      "Es gibt viele verschiedene Ausbildungsm철glichkeiten! F체r Menschen mit k철rperlichen Einschr채nkungen oder Handicaps bietet sich ein Praktikum im Rahmen eines Volontariats an, z.B. in einem fachspezifischen Betrieb oder Werkstatt oder Inklusionsbetrieb (f체r Menschen mit Lernschwierigkeiten) Es gibt auch eine Reihe von freien Gewerben f체r die kein Bef채higungsnachweis erforderlich ist; Die aktuelle bundeseinheitliche Liste der freien Gewerbe sowie die Liste der reglementierten Gewerbe ist jeweils auf der Website des Bundesministeriums f체r Digitalisierung und Wirtschaft (BMDW) abrufbar.<dummy00007> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     786.13 ms\n",
      "llama_print_timings:      sample time =      35.56 ms /   165 runs   (    0.22 ms per token,  4640.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10525.22 ms /    98 tokens (  107.40 ms per token,     9.31 tokens per second)\n",
      "llama_print_timings:        eval time =   45467.14 ms /   164 runs   (  277.24 ms per token,     3.61 tokens per second)\n",
      "llama_print_timings:       total time =   56710.77 ms /   262 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<dummy00007> \\n<dummy00006> assistant\\nEs gibt viele verschiedene Ausbildungsm철glichkeiten! F체r Menschen mit k철rperlichen Einschr채nkungen oder Handicaps bietet sich ein Praktikum im Rahmen eines Volontariats an, z.B. in einem fachspezifischen Betrieb oder Werkstatt oder Inklusionsbetrieb (f체r Menschen mit Lernschwierigkeiten) Es gibt auch eine Reihe von freien Gewerben f체r die kein Bef채higungsnachweis erforderlich ist; Die aktuelle bundeseinheitliche Liste der freien Gewerbe sowie die Liste der reglementierten Gewerbe ist jeweils auf der Website des Bundesministeriums f체r Digitalisierung und Wirtschaft (BMDW) abrufbar.<dummy00007> \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=new_prompt)\n",
    "\n",
    "chain.run(\"Hallo, ich bin mike, 15 jahre alt und m철chte gerne eine ausbildung machen. Ich war nicht so gut in der Schule, aber ich bin handwerklich begabt. Was f체r Berufe gibt es da?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
