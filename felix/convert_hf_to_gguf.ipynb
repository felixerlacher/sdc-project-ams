{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the Basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/llm/ChatTable/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "README.md: 100%|██████████| 2.71k/2.71k [00:00<00:00, 3.96MB/s]\n",
      "\n",
      "imgs/benchmarks.png: 100%|██████████| 42.2k/42.2k [00:00<00:00, 442kB/s]\n",
      "\n",
      "imgs/training_params.png: 100%|██████████| 97.3k/97.3k [00:00<00:00, 909kB/s]\n",
      ".gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 10.4MB/s]\n",
      "Fetching 13 files: 100%|██████████| 13/13 [00:17<00:00,  1.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/felix/llm/sdc-project-ams/felix/convert_models/leo-mistral-hessianai-7b'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_id = \"LeoLM/leo-mistral-hessianai-7b\"\n",
    "snapshot_download(repo_id=model_id, local_dir=\"convert_models/leo-mistral-hessianai-7b\", local_dir_use_symlinks=False, revision=\"main\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python llama.cpp/convert.py leo-hf \\\n",
    "  --outfile leo-7b.gguf \\\n",
    "  --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from leo-7b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 2.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32128]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32128]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32128]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 387/32128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.5\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  6829.70 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 0.5\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    70.75 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"convert_models/leo-7b.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-7456b837-4156-4628-b080-4b03663690b0', 'object': 'text_completion', 'created': 1705616592, 'model': 'leo-7b.gguf', 'choices': [{'text': '<|im_start|>system\\nDu bist hilfst bei der Berufsberatung. Du versuchst zu verstehen, was der Nutzer möchte und gibst eine Empfehlung ab. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein.<|im_end|>\\n<|im_start|>user\\nIch bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.<|im_end|>\\n<|im_start|>you\\nEs gibt viele verschiedene Berufe, die diese Beschreibung erfüllen. Einige Vorschläge sind: Kundenservicemitarbeiter, Lehrer, Psychologe und Sozialarbeiter. Jeder dieser Berufe erfordert mindestens einen High-School-Abschluss (Kundenservice und Lehrer möglicherweise auch ein Bachelor-Abschluss) sowie eine entsprechende Ausbildung in einem bestimmten Bereich.<|im_end|>\\n<|im_start|>user\\nWelcher Beruf passt am besten zu mir? Ich denke, ich habe das Zeug dazu, aber ich bin nicht sicher.<dummy00007> \\n<dummy00006> assistant\\nEs ist schwer zu sagen, welcher dieser Berufe für Sie am besten geeignet wäre, ohne weitere Informationen über Ihre Fähigkeiten und Interessen zu haben. Aber hier sind einige Dinge, die Sie beachten sollten:<|im_end|>\\n* Kundenservicemitarbeiter müssen hervorragende Kommunikationsfähigkeiten haben, da sie mit Menschen aus allen Lebensbereichen interagieren.<|im_start|>you\\nEinige Fähigkeiten, die für diesen Beruf erforderlich sind, umfassen Empathie, Geduld und ein freundliches Wesen. Dies ist normalerweise der Bereich, in dem Sie am meisten Erfahrung haben werden, da viele Unternehmen Praktika oder Teilzeitjobs für Teenager anbieten.<|im_end|>\\n* Lehrer müssen gut mit Kindern umgehen können und geduldig', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 150, 'completion_tokens': 362, 'total_tokens': 512}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1513.23 ms\n",
      "llama_print_timings:      sample time =      64.24 ms /   362 runs   (    0.18 ms per token,  5634.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1513.04 ms /   150 tokens (   10.09 ms per token,    99.14 tokens per second)\n",
      "llama_print_timings:        eval time =   38642.49 ms /   361 runs   (  107.04 ms per token,     9.34 tokens per second)\n",
      "llama_print_timings:       total time =   40699.08 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"<|im_start|>system\\nDu bist hilfst bei der Berufsberatung. Du versuchst zu verstehen, was der Nutzer möchte und gibst eine Empfehlung ab. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein.<|im_end|>\\n<|im_start|>user\\nIch bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.<|im_end|>\\n\", max_tokens=8000, stop=[\"\\n\"], echo=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from leo-7b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32              = 2.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32128]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32128]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32128]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 387/32128 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.5\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  6829.70 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 0.5\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    70.75 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"convert_models/leo-7b.gguf\", chat_format=\"llama-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-82eff731-85ed-4ef9-9e59-cc5bcfaeb026', 'object': 'chat.completion', 'created': 1705616492, 'model': 'leo-7b.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '<dummy00007> \\n<dummy00006> assistant\\nBasierend auf deinen Interessen könntest du eine Karriere als Kundendienstmitarbeiter oder Verkäufer in Betracht ziehen. Beide Berufe beinhalten regelmäßigen Umgang mit Kunden und das Helfen anderer Menschen bei der Lösung von Problemen oder dem Kauf von Produkten/Dienstleistungen.\\n\\nIn Bezug auf die Ausbildung benötigst du für beide Berufe in der Regel nur einen High-School-Abschluss, obwohl einige Arbeitgeber möglicherweise zusätzliche Schulungen oder Zertifizierungen bevorzugen. Es gibt viele Möglichkeiten zur Beschäftigung sowohl im Einzelhandel als auch am Telefon/online und das Gehalt kann je nach Erfahrung, Standort und Unternehmen variieren, aber es ist in der Regel ein fairer Lohn für einen guten Job mit viel Sozialkontakt.\\n\\nLetztendlich könntest du deine Berufsausichten erweitern, indem du zusätzliche Schulungen oder Kurse absolvierst (zum Beispiel im Bereich Management), um mehr Verantwortung und höhere Gehälter zu verfolgen.<dummy00007> \\n'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 137, 'completion_tokens': 269, 'total_tokens': 406}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1543.78 ms\n",
      "llama_print_timings:      sample time =      43.31 ms /   270 runs   (    0.16 ms per token,  6234.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1543.69 ms /   137 tokens (   11.27 ms per token,    88.75 tokens per second)\n",
      "llama_print_timings:        eval time =   28767.73 ms /   269 runs   (  106.94 ms per token,     9.35 tokens per second)\n",
      "llama_print_timings:       total time =   30710.40 ms /   406 tokens\n"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Du bist hilfst bei der Berufsberatung. Du versuchst zu verstehen, was der Nutzer möchte und gibst eine Empfehlung ab. Gehe auf Tätigkeitsmerkmale, Anforderungen, Ausbildung, Beschäftigungsmöglichkeiten, Gehalt und Berufsausichten ein.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Ich bin 15 Jahre alt, gut im Umgang mit Menschen und möchte gerne einen Beruf erlernen, in dem ich viel mit Menschen zu tun habe.\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Basemodel + Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import transformers\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"./convert_models/leo-mistral-hessianai-7b\"\n",
    "# model_name = model.split(\"/\")[-1]\n",
    "adapter_name = \"./LeoLM/leo-mistral-hessianai-7b-ams-finetuned/\"\n",
    "#adapter_name = f\"{model_name}-ams-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:23<00:00, 71.92s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        max_memory={i: '24000MB' for i in range(torch.cuda.device_count())},\n",
    "        # slower?\n",
    "        #load_in_4bit=True,\n",
    "        #load_in_8bit=True,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "        # quantization_config=BitsAndBytesConfig(\n",
    "        #     load_in_4bit=True,\n",
    "        #     bnb_4bit_compute_dtype=torch.float16,\n",
    "        #     bnb_4bit_use_double_quant=True,\n",
    "        #     bnb_4bit_quant_type=\"nf4\",        \n",
    "        # )\n",
    "    )\n",
    "model = PeftModel.from_pretrained(model, adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"leo-mistral-hessianai-7b-ams-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert merged Model to GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy tokenizer_config.json, tokenizer.json and tokenizer.model from base model to merged model to fix vocab error {f16, q8_0}\n",
    "\n",
    "python convert_models/llama.cpp/convert.py LeoLm/leo-mistral-hessianai-7b-ams-merged \\\n",
    "  --outfile LeoLm/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf \\\n",
    "  --outtype f16 --pad-vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize model further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python convert_models/llama.cpp/convert.py LeoLm/leo-mistral-hessianai-7b-ams-merged \\\n",
    "  --outfile LeoLm/leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf \\\n",
    "  --outtype f16\n",
    "\n",
    "quantize leo-mistral-hessianai-7b-ams-merged-16bit-f16.gguf leo-mistral-hessianai-7b-ams-merged-16bit-Q4_K_M.gguf Q4_K_M"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
